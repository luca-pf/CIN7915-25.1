{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1ad4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Sentimental Analysis Algorithm - Algoritmo de Análise de Sentimento\n",
    "%pip install torch --quiet\n",
    "%pip install pandas scikit-learn nltk --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea4c5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Luca\n",
      "[nltk_data]     Flores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Luca\n",
      "[nltk_data]     Flores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Luca\n",
      "[nltk_data]     Flores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to C:\\Users\\Luca\n",
      "[nltk_data]     Flores\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === IMPORTAÇÕES ===\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import RSLPStemmer\n",
    "import re\n",
    "import webbrowser\n",
    "import pickle\n",
    "\n",
    "# Downloads necessários para o NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f0e466ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. PRÉ-PROCESSAMENTO DE TEXTO ===\n",
    "def preprocess_text(text):\n",
    "    # Converte para minúsculas, remove pontuação, tokeniza, remove stopwords e aplica stemmer\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text, language='portuguese')\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    stemmer = RSLPStemmer()\n",
    "    tokens = [stemmer.stem(w) for w in tokens if w not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# === 2. CARGA E TRATAMENTO DOS DADOS ===\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path, encoding='utf-8')\n",
    "    df.dropna(inplace=True)\n",
    "    df['tokens'] = df['texto'].apply(preprocess_text)  # Aplica o pré-processamento\n",
    "    return df\n",
    "\n",
    "# Cria vocabulário baseado na frequência mínima dos tokens\n",
    "def build_vocab(token_lists, min_freq=1):\n",
    "    freq = {}\n",
    "    for tokens in token_lists:\n",
    "        for token in tokens:\n",
    "            freq[token] = freq.get(token, 0) + 1\n",
    "    vocab = {word: i + 1 for i, (word, count) in enumerate(freq.items()) if count >= min_freq}\n",
    "    vocab['<UNK>'] = 0  # Token desconhecido\n",
    "    return vocab\n",
    "\n",
    "# Codifica uma lista de tokens para uma sequência de índices do vocabulário\n",
    "def encode_tokens(tokens, vocab, max_len=50):\n",
    "    indices = [vocab.get(t, 0) for t in tokens]\n",
    "    return indices[:max_len] + [0] * max(0, max_len - len(indices))\n",
    "\n",
    "\n",
    "# === 3. CLASSE DE DATASET PERSONALIZADA ===\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, vocab, label_encoder):\n",
    "        self.inputs = [encode_tokens(t, vocab) for t in df['tokens']]\n",
    "        self.labels = label_encoder.transform(df['sentimento'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "# === 4. DEFINIÇÃO DO MODELO ===\n",
    "# Rede neural feedforward simples (MLP — Perceptron Multicamadas)\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, hidden_dim,dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, \n",
    "                            batch_first=True) # LSTM opcional, mas não usado no original\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)  # lstm_out: (batch, seq_len, hidden_dim)\n",
    "        pooled = lstm_out[:, -1, :]       # pega o último hidden state\n",
    "        # pooled = embedded.mean(dim=1)  # Média dos embeddings (pooling)\n",
    "        dropped = self.dropout(pooled)\n",
    "        return self.fc(dropped)\n",
    "\n",
    "\n",
    "# === 5. TREINAMENTO DO MODELO ===\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device, label_encoder):\n",
    "    model.to(device)\n",
    "\n",
    "    # for epoch in range(epochs):\n",
    "    #     model.train()\n",
    "    #     total_loss = 0\n",
    "    #     for x_batch, y_batch in train_loader:\n",
    "    #         x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "    #         optimizer.zero_grad()\n",
    "    #         outputs = model(x_batch)\n",
    "    #         loss = criterion(outputs, y_batch)\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         total_loss += loss.item()\n",
    "\n",
    "    #     # Validação\n",
    "    #     model.eval()\n",
    "    #     all_preds, all_labels = [], []\n",
    "    #     with torch.no_grad():\n",
    "    #         for x_val, y_val in val_loader:\n",
    "    #             x_val = x_val.to(device)\n",
    "    #             outputs = model(x_val)\n",
    "    #             preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    #             all_preds.extend(preds)\n",
    "    #             all_labels.extend(y_val.numpy())\n",
    "\n",
    "    #     epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    #     print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}, Accuracy: {epoch_acc:.2f}\")\n",
    "    #     print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_, zero_division=0))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_train = 0                # <-- Adicionado: conta acertos no treino\n",
    "        total_train = 0                  # <-- Adicionado: conta exemplos no treino\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)           # <-- Adicionado: calcula predições no treino\n",
    "            correct_train += (preds == y_batch).sum().item()# <-- Adicionado: soma acertos no treino\n",
    "            total_train += y_batch.size(0)                  # <-- Adicionado: soma total de exemplos no treino\n",
    "\n",
    "        train_acc = correct_train / total_train             # <-- Adicionado: calcula acurácia de treino\n",
    "\n",
    "        # Validação\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        val_loss = 0                                       # <-- Adicionado: soma loss de validação\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                outputs = model(x_val)\n",
    "                loss = criterion(outputs, y_val)            # <-- Adicionado: calcula loss de validação\n",
    "                val_loss += loss.item()                     # <-- Adicionado: soma loss de validação\n",
    "                preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(y_val.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(all_labels, all_preds)     # <-- Adicionado: calcula acurácia de validação\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, \"\n",
    "            f\"Train Loss: {total_loss / len(train_loader):.4f}, Train Acc: {train_acc:.2f}, \"\n",
    "            f\"Val Loss: {val_loss / len(val_loader):.4f}, Val Acc: {val_acc:.2f}\") # <-- Adicionado: imprime tudo\n",
    "    \n",
    "\n",
    "    # Relatório final\n",
    "    final_report = classification_report(all_labels, all_preds, target_names=label_encoder.classes_, output_dict=True, zero_division=0)\n",
    "    final_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    html_report = classification_report_to_html(final_report, label_encoder.classes_, final_accuracy, conf_matrix)\n",
    "\n",
    "    html_path = 'resultado_validacao.html'\n",
    "    with open(html_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_report)\n",
    "\n",
    "    print(f'Relatório final salvo em {html_path}')\n",
    "    webbrowser.open(f'file://{os.path.abspath(html_path)}')\n",
    "\n",
    "    # Salva modelo treinado\n",
    "    torch.save(model.state_dict(), 'modelo_treinado.pth')\n",
    "    print(\"Modelo salvo como modelo_treinado.pth\")\n",
    "\n",
    "\n",
    "# === CONVERSÃO DE RELATÓRIO PARA HTML ===\n",
    "def classification_report_to_html(report_dict, class_names, accuracy, conf_matrix):\n",
    "    html = f\"\"\"\n",
    "    <html><head><title>Relatório de Validação</title></head>\n",
    "    <body>\n",
    "    <h1>Relatório de Classificação</h1>\n",
    "    <h2>Acurácia Final: {accuracy:.2f}</h2>\n",
    "\n",
    "    <h3>Métricas por Classe</h3>\n",
    "    <table border=\"1\" cellpadding=\"8\">\n",
    "    <tr><th>Classe</th><th>Precisão</th><th>Recall</th><th>F1-score</th><th>Suporte</th></tr>\n",
    "    \"\"\"\n",
    "    for label in class_names:\n",
    "        metrics = report_dict[label]\n",
    "        html += f\"<tr><td>{label}</td><td>{metrics['precision']:.2f}</td><td>{metrics['recall']:.2f}</td><td>{metrics['f1-score']:.2f}</td><td>{metrics['support']}</td></tr>\"\n",
    "    html += \"</table>\"\n",
    "\n",
    "    # Matriz de confusão\n",
    "    html += \"<h3>Matriz de Confusão</h3><table border='1' cellpadding='8'><tr><th></th>\"\n",
    "    for label in class_names:\n",
    "        html += f\"<th>{label}</th>\"\n",
    "    html += \"</tr>\"\n",
    "    for i, row in enumerate(conf_matrix):\n",
    "        html += f\"<tr><th>{class_names[i]}</th>\"\n",
    "        for val in row:\n",
    "            html += f\"<td>{val}</td>\"\n",
    "        html += \"</tr>\"\n",
    "    html += \"</table>\"\n",
    "\n",
    "    html += \"</body></html>\"\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a08eb4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16, Train Loss: 1.1515, Train Acc: 0.31, Val Loss: 1.0980, Val Acc: 0.25\n",
      "Epoch 2/16, Train Loss: 1.2096, Train Acc: 0.31, Val Loss: 1.1028, Val Acc: 0.25\n",
      "Epoch 3/16, Train Loss: 1.0786, Train Acc: 0.44, Val Loss: 1.1121, Val Acc: 0.25\n",
      "Epoch 4/16, Train Loss: 1.1834, Train Acc: 0.25, Val Loss: 1.0921, Val Acc: 0.25\n",
      "Epoch 5/16, Train Loss: 1.2004, Train Acc: 0.25, Val Loss: 1.0910, Val Acc: 0.50\n",
      "Epoch 6/16, Train Loss: 1.1065, Train Acc: 0.44, Val Loss: 1.0951, Val Acc: 0.25\n",
      "Epoch 7/16, Train Loss: 1.1411, Train Acc: 0.31, Val Loss: 1.0925, Val Acc: 0.50\n",
      "Epoch 8/16, Train Loss: 1.1021, Train Acc: 0.31, Val Loss: 1.0875, Val Acc: 0.50\n",
      "Epoch 9/16, Train Loss: 1.1494, Train Acc: 0.19, Val Loss: 1.0743, Val Acc: 0.50\n",
      "Epoch 10/16, Train Loss: 1.1335, Train Acc: 0.31, Val Loss: 1.0627, Val Acc: 0.50\n",
      "Epoch 11/16, Train Loss: 1.1099, Train Acc: 0.44, Val Loss: 1.0666, Val Acc: 0.50\n",
      "Epoch 12/16, Train Loss: 1.0915, Train Acc: 0.38, Val Loss: 1.0577, Val Acc: 0.50\n",
      "Epoch 13/16, Train Loss: 1.1411, Train Acc: 0.38, Val Loss: 1.0638, Val Acc: 0.50\n",
      "Epoch 14/16, Train Loss: 1.1283, Train Acc: 0.31, Val Loss: 1.0637, Val Acc: 0.50\n",
      "Epoch 15/16, Train Loss: 1.1259, Train Acc: 0.38, Val Loss: 1.0622, Val Acc: 0.50\n",
      "Epoch 16/16, Train Loss: 1.1400, Train Acc: 0.38, Val Loss: 1.0700, Val Acc: 0.50\n",
      "Relatório final salvo em resultado_validacao.html\n",
      "Modelo salvo como modelo_treinado.pth\n",
      "Vocabulário salvo como vocab.pkl\n",
      "LabelEncoder salvo como label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# === 6. EXECUÇÃO PRINCIPAL ===\n",
    "def main():\n",
    "    filepath = r\"C:\\Users\\Luca Flores\\Downloads\\posts.csv\"  # Caminho para seu dataset\n",
    "    df = load_data(filepath)\n",
    "\n",
    "    vocab = build_vocab(df['tokens'])\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(df['sentimento'])\n",
    "\n",
    "    dataset = TextDataset(df, vocab, label_encoder)\n",
    "\n",
    "    # Divisão simples: primeiros 16 para treino, 4 para validação\n",
    "    train_indices = list(range(16))\n",
    "    val_indices = list(range(16, 20))\n",
    "\n",
    "    train_data = Subset(dataset, train_indices)\n",
    "    val_data = Subset(dataset, val_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=4)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SentimentClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=25,\n",
    "        num_classes=len(label_encoder.classes_),\n",
    "        hidden_dim=16,  # Dimensão do hidden state da LSTM\n",
    "        dropout=0.5\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.9)\n",
    "\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, epochs=16, device=device,\n",
    "                label_encoder=label_encoder)\n",
    "\n",
    "    # Salva vocabulário\n",
    "    with open('vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "\n",
    "    # Salva codificador de labels\n",
    "    with open('label_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "\n",
    "    print(\"Vocabulário salvo como vocab.pkl\")\n",
    "    print(\"LabelEncoder salvo como label_encoder.pkl\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SARC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
